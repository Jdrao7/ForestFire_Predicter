


import pandas as pd


df=pd.read_csv("Algerian_forest_fires_dataset_UPDATE.csv",header=1)


df1=df.iloc[0:121]
df2=df.iloc[125:]


df2.isnull().sum()


df=pd.concat([df1,df2],ignore_index=True)


df.columns=df.columns.str.strip()


df['Classes'].value_counts()


df.drop(['day','year'],axis=1,inplace=True)


df.isnull().sum()


df.dropna(inplace=True)


df.isnull().sum()


cols_to_convert = df.columns[:-8]

# convert columns to integer type
df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric,errors='coerce').astype(int)


df.head()


df.info()


df['Classes'].unique()


df['Classes']=df['Classes'].apply(lambda x:1 if x.strip()=='fire' else 0)


df['Classes'].unique()


df.info()


columns_to_float=df.columns[-8:-1]


columns_to_float


df[columns_to_float]=df[columns_to_float].astype(float)


df.info()





df.corr()


dropped_features=set()
corr_matrix=df.corr().abs()
threshold=0.85


for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if corr_matrix.iloc[i,j]>threshold:
            col_j=corr_matrix.columns[j]
            if col_j not in dropped_features:
                dropped_features.add(col_j)

df.drop(columns=dropped_features,inplace=True)


dropped_features


df['Rain'] = df['Rain'].apply(lambda x: 0 if x == 0 else 1)


df.columns


from sklearn.linear_model import LogisticRegression


model=LogisticRegression(max_iter=10000)


X=df.iloc[:,:-1]
y=df.iloc[:,-1]


from sklearn.model_selection import train_test_split


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=45)








from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame with original columns as without it the pred accuracy in numpy array form is coming 55% idk why?
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)


from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)


model.fit(X_train_scaled,y_train)


df.corr()


y_pred=model.predict(X_test_scaled)


import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report


accuracy_score(y_pred,y_test)


confusion_matrix(y_pred,y_test)


classification_report(y_pred,y_test)





penalty=['l1','l2']
c_values=[100,10,1.0,0.1,0.01]
solver=['saga']


params=dict(penalty=penalty,C=c_values,solver=solver) # list inside dict


from sklearn.model_selection import StratifiedKFold


cv=StratifiedKFold()


from sklearn.model_selection import GridSearchCV


grid=GridSearchCV(estimator=model,param_grid=params,scoring='accuracy',cv=cv,n_jobs=-1)


grid.fit(X_train_scaled,y_train)


grid.best_score_


model_grid=grid.best_estimator_


y_pred_grid=model_grid.predict(X_test_scaled)


accuracy_score(y_pred_grid,y_test)


param_grid = [
    {'penalty': ['l1'], 'C': [0.01, 0.1, 1], 'solver': ['saga','liblinear']},
    {'penalty': ['l2'], 'C': [0.01, 0.1, 1], 'solver': ['saga','liblinear','lbfgs','newton-cg','sag']},
    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1], 'solver': ['saga'], 'l1_ratio': [0.0, 0.5, 1.0]}
] # dict inside list


grid2=GridSearchCV(estimator=model,param_grid=param_grid,scoring='accuracy',cv=cv,n_jobs=-1)


grid2.fit(X_train_scaled,y_train)


grid2.best_score_





import pickle


with open('model.pkl','wb') as f:
    pickle.dump(grid2.best_estimator_,f)





model.coef_


model.coef_


df.columns


feature_names = X.columns
coefficients = model.coef_[0]  

for feat, coef in zip(feature_names, coefficients):
    if feat == 'Rain':
        print(f"Rain coefficient: {coef}")


print(df['rain_binary'].value_counts()) # As you can see the rain is positively related with fires which is absurd need to drop Rain but its also an important feature for now just covertinf it into binary classsification


import seaborn as sns
sns.countplot(x='rain_binary', hue='Classes', data=df)



